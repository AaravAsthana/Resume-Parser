# Resume Parser Pro

A local/streamlitâ€based resume parsing application with:

- PDF text + hyperlink extraction (PyMuPDF, pdfplumber, OCR fallback)  
- Section detection (experience, education, skills, etc.)  
- Name, email, phone, LinkedIn & GitHub link extraction via regex & NER  
- Skill extraction & optional companyâ€‘supplied or JDâ€‘derived keyword matching  
- ATS score calculation & animated gauge/bar display  
- Companyâ€“Position pair extraction (regex + Google Gemini fallback)  
- Export JSON, Excel; interactive perâ€‘resume viewer in Streamlit  
- Dockerfile + Poetry for reproducible local or container use  

---

## ğŸ“ Project Layout

```text
Resume_Parser/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ poetry.lock
â”œâ”€â”€ app.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extractor.py
â”‚   â””â”€â”€ pipeline.py
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ logo.png
â”œâ”€â”€ requirements.txt
â””â”€â”€ resumes/           # drop your PDF resumes here
```
## ğŸš€ Quickstart

### ğŸ“¦ Installation

1.  **Clone the repo**
    
    ```bash
    `git clone https://github.com/AaravAsthana/Resume-Parser.git cd Resume-Parser` 
    ```
2.  **Create & activate a virtualenv**
    
    ```bash
    `python3.12 -m venv .venv source .venv/bin/activate # macOS/Linux .venv\Scripts\activate # Windows PowerShell` 
    ```
3.  **Install dependencies**
    
    ```bash
    `pip install --upgrade pip
    pip install -r requirements.txt` 
    
    > This will pull in everythingâ€”including the spaCy model wheel listed in `requirements.txt`â€”so you wonâ€™t need to manually `python -m spacy download en_core_web_sm`.
    ```
4.  **(Optional) Docker**  
    If you prefer Docker:
    
    ```bash
    docker build -t resume-parser:latest .
    docker run -p 8501:8501 resume-parser:latest 
    ```
    Then open [http://localhost:8501](http://localhost:8501) in your browser.
    

----------

## ğŸ”‘ Environment Variables

-   Copy `.env.example` to `.env` and fill in:
    
   ``` ini    
    GOOGLE_API_KEY=your-api-key 
  ```  
-   **On Streamlit Cloud**, add the same key under **Settingsâ€¯â†’â€¯Secrets** as TOML:
    
 ```toml
  GOOGLE_API_KEY = "your-api-key"
 ```
----------
## ğŸš€ Running Locally

With pip:

```bash
streamlit run app.py
```

With Poetry:
```bash
poetry run streamlit run app.py
```
Then open http://localhost:8501 in your browser. 

----------

## ğŸ³ Docker

1.  **Build image** (must have `Dockerfile` in project root):
    
    ```bash
    `docker build -t resume-parser-image:latest .` 
    ```
2.  **Run container** (map port, pass API key):
    
   ```bash
    docker run -p 8501:8501 \
	    -e GOOGLE_API_KEY="$GOOGLE_API_KEY" \
	     resume-parser-image:latest 
```
3.  Browse to [http://localhost:8501](http://localhost:8501).

    > _(For futureâ€proofing)_

```dockerfile
# Dockerfile snippet
FROM python:3.12-slim
WORKDIR /app
COPY . .
RUN apt-get update && apt-get install -y tesseract-ocr libgl1-mesa-glx && \
    pip install --no-cache-dir -r requirements.txt
EXPOSE 8501
CMD ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"] 
```

----------

## âš™ï¸ Configuration & Usage

-   **Company Inputs**: enter commaâ€‘separated â€œrequired skillsâ€ or upload a plainâ€‘text JD.
    
-   **Upload Resumes**: drop one or more PDFs, click **Process Resumes**.
    
-   **Sidebar**: shows progress bar, clickable list of parsed resumes.
    
-   **Main View**: perâ€‘resume PDF embed + animated ATS gauge/bar + details table + perâ€‘field scroll for long text.
    
-   **Download**: Excel export of all parsed resumes or singleâ€‘resume CSV view.
    

----------

## ğŸ› ï¸ How It Works

1.  **extractor.py**
    
    -   `extract_text_and_links()` pulls text & link URIs from PDF (PyMuPDF), falls back to `pdfplumber` or OCR.
        
    -   Regex + spaCy for name, email, phone, LinkedIn/GitHub.
        
    -   Section split via heading keywords.
        
    -   Regex + Google Gemini to pull Companyâ€“Position pairs.
        
2.  **pipeline.py**
    
    -   Invokes extractor, builds skill list.
        
    -   Loads â€œrequired skillsâ€ (UI input or `company_skills.json`) + JD phrases.
        
    -   Matches resume skills vs. required set â†’ ATS score %.
        
    -   Returns structured dict per resume.
        
3.  **app.py**
    
    -   Streamlit frontâ€‘end, PDF embed, controls, animated gauge/bar, table UI, download buttons.
        

----------
## ğŸ“ Whatâ€™s Inside

- `extractor.py` / `pipeline.py`: Resume parsing logic, company skill & JD keyword matching, ATSâ€‘score calculation.

- `app.py`: Streamlit UI with file uploader, ATS donut animation, resume viewer, and downloadable Excel.

- `requirements.txt`: All PyPI dependencies including spaCyâ€™s `en_core_web_sm` wheel.

- `pyproject.toml`: Poetry manifest, if you prefer that route.
----------
## ğŸ“ Future / Docker Tips

-   **Adding README.md** ensures Poetry can package the project (otherwise use `--no-root`).
    
-   To rebuild after code changes:
    
    ```bash
    docker build -t resume-parser-image:latest .
    docker run -p 8501:8501 -e GOOGLE_API_KEY="$GOOGLE_API_KEY" resume-parser-image:latest 
    ```
-   **Common pitfalls**:
    
    -   `.env` not loaded â†’ missing API key errors
        
    -   spaCy model not installed â†’ run `spacy download en_core_web_sm`
        
    -   Docker context missing `Dockerfile` â†’ ensure youâ€™re in project root
        

----------
## ğŸ¤ Contributing
Feel free to open issues or PRs. If you add new features, please update this README accordingly.

----------
## ğŸ“„ License

MIT Â© 2025 Aarav Asthana
